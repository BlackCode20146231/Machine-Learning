{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhhQIHPYPg9WcqB/brNF/N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blowmeaway1234/Machine-Learning/blob/main/CHAPTER_9_Dimensionality_Reduction_Using_Feature_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.1 Reducing Features Using Principal Components"
      ],
      "metadata": {
        "id": "ELlE50oSMsT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem**\n",
        "Given a set of features, you want to reduce the number of features while retaining the\n",
        "variance in the data.\n",
        "\n",
        "**Solution**\n",
        "Use principal component analysis with scikit’s PCA:"
      ],
      "metadata": {
        "id": "6GhNSzLgMw6U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcMOd9UCMqa5",
        "outputId": "21fc77d3-a9d9-49bc-9c34-aabcca39868a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 64\n",
            "Reduced number of features: 54\n"
          ]
        }
      ],
      "source": [
        "# Load libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import datasets\n",
        "# Load the data\n",
        "digits = datasets.load_digits()\n",
        "# Standardize the feature matrix\n",
        "features = StandardScaler().fit_transform(digits.data) #->Chuẩn hóa dữ liệu\n",
        "# Create a PCA that will retain 99% of variance\n",
        "pca = PCA(n_components=0.99, whiten=True) #-> thuật toán PCA\n",
        "# Conduct PCA\n",
        "features_pca = pca.fit_transform(features)\n",
        "# Show results\n",
        "print(\"Original number of features:\", features.shape[1]) # Ban đầu là 64\n",
        "print(\"Reduced number of features:\", features_pca.shape[1]) # với thuật toán PCA giảm đc 15 % \n",
        "# PCA giảm chiều dữ liệu phi tuyến tính"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.2 Reducing Features When Data Is Linearly Inseparable"
      ],
      "metadata": {
        "id": "28eWC-OLRMsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem**\n",
        "\n",
        "You suspect you have linearly inseparable data and want to reduce the dimensions.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Use an extension of principal component analysis that uses kernels to allow for non\u0002linear dimensionality reduction:"
      ],
      "metadata": {
        "id": "MlES8L0TRQ0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "from sklearn.datasets import make_circles\n",
        "# Create linearly inseparable data\n",
        "features, _ = make_circles(n_samples=1000, random_state=1, noise=0.1, factor=0.1)\n",
        "# Apply kernal PCA with radius basis function (RBF) kernel\n",
        "kpca = KernelPCA(kernel=\"rbf\", gamma=15, n_components=1)\n",
        "features_kpca = kpca.fit_transform(features)\n",
        "print(\"Original number of features:\", features.shape[1])\n",
        "print(\"Reduced number of features:\", features_kpca.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6VLgwaCRQMJ",
        "outputId": "61d829a8-e104-41e1-8504-c16f93418899"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 2\n",
            "Reduced number of features: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.3 Reducing Features by Maximizing Class Separability"
      ],
      "metadata": {
        "id": "6xK0c0XRRZv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem**\n",
        "\n",
        "You want to reduce the features to be used by a classifier.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Try linear discriminant analysis (LDA) to project the features onto component axes\n",
        "that maximize the separation of classes:"
      ],
      "metadata": {
        "id": "ePC0FFc6Rd2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# Load Iris flower dataset:\n",
        "iris = datasets.load_iris()\n",
        "features = iris.data\n",
        "target = iris.target\n",
        "# Create and run an LDA, then use it to transform the features\n",
        "lda = LinearDiscriminantAnalysis(n_components=1)\n",
        "features_lda = lda.fit(features, target).transform(features)\n",
        "# Print the number of features\n",
        "print(\"Original number of features:\", features.shape[1])\n",
        "print(\"Reduced number of features:\", features_lda.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMz-h_OzRbQ4",
        "outputId": "f6582456-c5d8-4d65-f7d3-6464aa8df882"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 4\n",
            "Reduced number of features: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda.explained_variance_ratio_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOfh1veRRhQY",
        "outputId": "002dd3ff-7b5a-4b1c-dd5c-d13f333045f1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9912126])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifically, we can run LinearDiscriminantAnalysis with n_components set to\n",
        "\n",
        "None to return the ratio of variance explained by every component feature, then cal‐\n",
        "culate how many components are required to get above some threshold of variance \n",
        "\n",
        "explained (often 0.95 or 0.99):\n"
      ],
      "metadata": {
        "id": "A79WcefnSJP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# Create and run LDA\n",
        "lda = LinearDiscriminantAnalysis(n_components=None)\n",
        "features_lda = lda.fit(features, target)\n",
        "# Create array of explained variance ratios\n",
        "lda_var_ratios = lda.explained_variance_ratio_\n",
        "# Create function\n",
        "def select_n_components(var_ratio, goal_var: float) -> int:\n",
        "  # Set initial variance explained so far\n",
        "  total_variance = 0.0\n",
        "  # Set initial number of features\n",
        "  n_components = 0\n",
        "  # For the explained variance of each feature:\n",
        "  for explained_variance in var_ratio:\n",
        "    # Add the explained variance to the total\n",
        "    total_variance += explained_variance\n",
        "    # Add one to the number of components\n",
        "    n_components += 1\n",
        "    # If we reach our goal level of explained variance\n",
        "    if total_variance >= goal_var:\n",
        "      # End the loop\n",
        "      break\n",
        "  # Return the number of components\n",
        "  return n_components\n",
        "# Run function\n",
        "select_n_components(lda_var_ratios, 0.95)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bS3oAKLRnjO",
        "outputId": "b9f3ef1e-1594-4468-c31a-98a6c50072db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.4 Reducing Features Using Matrix Factorization"
      ],
      "metadata": {
        "id": "32sT8daPSp3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem**\n",
        "\n",
        "You have a feature matrix of nonnegative values and want to reduce the dimensional‐\n",
        "ity.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "Use non-negative matrix factorization (NMF) to reduce the dimensionality of the fea‐\n",
        "ture matrix:"
      ],
      "metadata": {
        "id": "pNdT75OWSvJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn import datasets\n",
        "# Load the data\n",
        "digits = datasets.load_digits()\n",
        "# Load feature matrix\n",
        "features = digits.data\n",
        "# Create, fit, and apply NMF\n",
        "nmf = NMF(n_components=10, random_state=1)\n",
        "features_nmf = nmf.fit_transform(features)\n",
        "# Show results\n",
        "print(\"Original number of features:\", features.shape[1])\n",
        "print(\"Reduced number of features:\", features_nmf.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-wkvxfZS0Ss",
        "outputId": "d47beb8e-9e15-46a6-883c-a0dc2c3444f1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 64\n",
            "Reduced number of features: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.5 Reducing Features on Sparse Data"
      ],
      "metadata": {
        "id": "Fct7YBR2S4Af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem**\n",
        "\n",
        "You have a sparse feature matrix and want to reduce the dimensionality.\n",
        "\n",
        "**Solution**\n",
        "Use Truncated Singular Value Decomposition (TSVD):"
      ],
      "metadata": {
        "id": "EO_SkqlzS5sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "# Load the data\n",
        "digits = datasets.load_digits()\n",
        "# Standardize feature matrix\n",
        "features = StandardScaler().fit_transform(digits.data)\n",
        "# Make sparse matrix\n",
        "features_sparse = csr_matrix(features)\n",
        "# Create a TSVD\n",
        "tsvd = TruncatedSVD(n_components=10)\n",
        "# Conduct TSVD on sparse matrix\n",
        "features_sparse_tsvd = tsvd.fit(features_sparse).transform(features_sparse)\n",
        "# Show results\n",
        "print(\"Original number of features:\", features_sparse.shape[1])\n",
        "print(\"Reduced number of features:\", features_sparse_tsvd.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQF00Ly1S98V",
        "outputId": "8fbab9fc-f17d-4410-8e54-1e0512e2f3c5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 64\n",
            "Reduced number of features: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TSVD is similar to PCA and in fact, PCA actually often uses non-truncated Singular\n",
        "Value Decomposition (SVD) in one of its steps. In regular SVD, given d features,\n",
        "SVD will create factor matrices that are d × d, whereas TSVD will return factors that\n",
        "are n × n, where n is previously specified by a parameter. The practical advantage of\n",
        "TSVD is that unlike PCA, it works on sparse feature matrices."
      ],
      "metadata": {
        "id": "yJ1LAE-BTHAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum of first three components' explained variance ratios\n",
        "tsvd.explained_variance_ratio_[0:3].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDe9EwKdTH0m",
        "outputId": "5e8a64ae-0c06-4a1c-a4a5-8789070dbc09"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.30039385392832774"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can automate the process by creating a function that runs TSVD with n_compo\n",
        "nents set to one less than the number of original features and then calculate the num‐\n",
        "ber of components that explain a desired amount of the original data’s variance:"
      ],
      "metadata": {
        "id": "E1lZMqa_TKBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and run an TSVD with one less than number of features\n",
        "tsvd = TruncatedSVD(n_components=features_sparse.shape[1]-1)\n",
        "features_tsvd = tsvd.fit(features)\n",
        "# List of explained variances\n",
        "tsvd_var_ratios = tsvd.explained_variance_ratio_\n",
        "# Create a function\n",
        "def select_n_components(var_ratio, goal_var):\n",
        "  # Set initial variance explained so far\n",
        "  total_variance = 0.0\n",
        "  # Set initial number of features\n",
        "  n_components = 0\n",
        "  # For the explained variance of each feature:\n",
        "  for explained_variance in var_ratio:\n",
        "    # Add the explained variance to the total\n",
        "    total_variance += explained_variance\n",
        "    # Add one to the number of components\n",
        "    n_components += 1\n",
        "    # If we reach our goal level of explained variance\n",
        "    if total_variance >= goal_var:\n",
        "      # End the loop\n",
        "      break\n",
        "  # Return the number of components\n",
        "  return n_components\n",
        "# Run function\n",
        "select_n_components(tsvd_var_ratios, 0.95)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzgUvjslTKQU",
        "outputId": "c18be696-2ee4-4bf3-9a0b-c85605dee5f7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}